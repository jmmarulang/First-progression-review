%% ----------------------------------------------------------------
%% Introduction.tex
%% ---------------------------------------------------------------- 
\chapter{Problem Statement} \label{Chapter:Problem Statement}

%\jnote{Not sure if I should frame the text around symbolic computing vs subsymbolic computing (intersymbolic computing, Symbolic-numeric computating) or around symbolic vs subsymbolic AI (intersymbolic AI, neuro-symbolic AI). Will focus on AI for now}

%\jnote{Inspired on \cite{Platzer_2024}. The terms \SuAI{}, \SiAI{} and \InAI are taken from there.}

Broadly speaking, there exists  two very distinct approaches to \emph{\AILong{}}  (\emph{\AI{}}): One rooted in reasoning and another in learning \mcita{}.  In \citetalias{Platzer_2024}, \citeauthor{Platzer_2024} proposes the terms \emph{\SiAI{}} and \emph{\SuAI{}}, respectively. Here we define them a bit more generally. \SiAI{} (also referred to as good old fashioned AI \mcita{}, classical AI \mcita{},  or logic-based AI \mcita{}) relates to algebraic computing, and emphasizes finding analytical solutions by manipulating logical expressions. This approach, by principle, prioritizes interpretability and preserving meaning. Current relevant examples include \emph{SMT/SAT solvers} \mcita{}, \emph{theorem provers} \mcita{},  as well as many \emph{programming languages} (PL) \mcita{}.   \SiAI{} has had a broad impact on planning \mcita{}, gameplay \mcita{} and software/hardware verification \mcita{}, as well as on the field of mathematics \mcita{}.  On the other hand, \SuAI{} (also referred to as pattern engines \mcita{})

\jnote{I like the term "universal approximators", based on the universal approximation theorem that states that a neural network can approximate any continuous function to any desired degree of accuracy. I feel it better portrais what subsymbolic AI is, and would like to add it to the list of alternative names. But as far as I know its not been introduced before.}

relates to numerical computing,  and focuses on approximate solutions by applying statistical and optimization methods. They are often data driven, and do not require an explicit algorithm to operate (beyond the indirect computations performed to approximate the result). The more relevant examples of \SuAI{} are \emph{Machine Learning} (ML) \mcita{} and \emph{Reinforcement Learning} (RL) \mcita{}, but older methods such as \emph{Kalman Filters} \mcita{} could arguably also fall under this category.  \SuAI{} has recently grown in quality and proliferated to many applications such as image/language processing \mcita{}, and simulation \mcita{}. 

In order to leverage their respective strengths, there is a growing interest in studying and developing methods that merge Symbolic and Subsymbolic AI \mcita{}. This broad category, nicked \emph{\InAI{}} by \citeauthor{Platzer_2024} (also referred to as neuro-symbolic AI \mcita{}, or hybrid intelligent systems \mcita{}), can range from applying logical principles in the architecture of \emph{ \NN{}s} (NN)  \mcita{}, to using large language models to generate better heuristics for theorem provers \mcita{}. Even more, \InAI{} has found particular success in cyber-physical systems, where it often plays the role of a controller \citep{Platzer_2024}. 

Despite this, many \InAI{} systems often lack formal rigour, which prevents us from taking full advantage of its components. From the \SiAI{} perspective,  many symbolic principles are often haphazardly applied. Even more, many \InAI{} systems lack logical, algebraic or categorical representations, with proper syntax and semantics (for examples, see \mcita{}). Therefore, the interpretability and theoretical guarantees that symbolic systems benefit from end up being obfuscated. This is also exacerbated when dealing with \emph{\NN{}} (NN), widely considered to be black boxes \mcita{}. On the other hand, the statistical and probabilistic guarantees that are given for certain \SuAI{}  systems (such as robustness \mcita{} and generalization error boundaries \mcita{} ), as well as the effects that \SiAI{} principles may have on performance, remain understudied for \InAI{}.

Being so, during my PhD I aim to aid in the development of formal theories and guarantees for \InAI{}, applying techniques from both the Symbolic and Subsymbolic AI communities. Our current approach focuses on the study of \emph{\QL{}s} (QL), i.e.~logics that model the real numbers. They function as a bridge between formal logic and machine learning, and may provide a proper logical interpretation for \InAI{} systems.

To illustrate QLs, let us have a toy syntax with atomic propositions and conjunction, such as
\begin{equation}
\begin{split}
    \Phi \ni \phi &:= A \,|\, \phi \land \phi
\end{split}
\end{equation}
where $A$ is interpreted in a domain $D \subseteq \Ereal$. $D$ varies among logics and restricts the interpretation of connectives. For example, the
G\"{o}del logic \mcita{} has a standard semantics over $[0, 1]$ where the conjunction is interpreted as the minimum function.

Recently, there was a surge of interest in QLs, stimulated by the growing interest in \emph{AI safety} \mcita{}. \emph{\DL{}s} (DL) form a family of methods that apply key insights from quantitative logics to this domain for property-driven learning and safe-by-construction \InAI{} \mcita{}. Nevertheless, there is one fundamental problem that DLs face: Many specifications of interest for ML involve quantifiers, yet the majority of QLs are propositional \mcita{}.  Expanding some sound and complete propositional QLs to first-order logic often comes at the expense of either completeness or continuity \mcita{}, properties relevant from the perspectives of Symbolic and Subsymbolic AI, respectively \mcita{}.  Recently, a promising solution was proposed by \mcita{}: interpreting quantifiers as \textit{$p$-means} \mcita{}, a generalization of $p$-norms over a probability space \mcita{}.  

By leveraging these results, we are currently developing a first-order \DL{} with full theoretical rigour, to avoid the common shortcomings  of \InAI{} mentioned previously. Furthermore, we seek to formalize our results in the Rocq proof assistant, with the aid of the \mathcomp{} library. In this progression review, I present the state of the art of \yada, expand on our current approach, present some preliminary results,  and discuss my next steps. 

\jnote{Not sure what to put in the state of the art. The easiest thing to do would be to focus just on quantitative logics. But perhaps thats too limiting? I must also remember I must submit by the end of october...}