%% ----------------------------------------------------------------
%% Introduction.tex
%% ---------------------------------------------------------------- 
\chapter{Problem Statement} \label{Chapter:Problem Statement}

Broadly speaking, there exists  two very distinct approaches to \emph{\AILong{}}  (\emph{\AI{}}): One rooted in reasoning and another in learning \citep{Platzer_2024, booch2021thinking}.  \SiAI{} (also referred to as good old fashioned AI \citep{haugeland1989artificial}, classical AI \citep{garnelo2019reconciling},  or logic-based AI \citep{thomason2003logic}) relates to algebraic computing, and emphasizes finding analytical solutions by manipulating logical expressions. This approach, by principle, prioritizes interpretability and preserving meaning. Current relevant examples include \emph{SMT/SAT solvers} \citep{barrett2018satisfiability, alyahya2022structure}, \emph{theorem provers} \citep{bartek2025vampire, barras1999coq},  as well as many \emph{programming languages} (PL) \citep{korner2022fifty, perkel2019julia, klabnik2023rust}.
\jnote{Not sure about what to reference for programming languages} \SiAI{} has had a broad impact on planning \citep{geffner2013concise}, gameplay \citep{newell1958chess} and system verification \citep{leroy2016compcert, tihanyi2025new}, as well as on the field of mathematics \cite{Blokpoel2024}.  On the other hand, \SuAI{} (also referred to as pattern engines \citep{julia2020there})
%\jnote{I like the term "universal approximators", based on the universal approximation theorem that states that a neural network can approximate any continuous function to any desired degree of accuracy. I feel it better portrais what subsymbolic AI is, and would like to add it to the list of alternative names. But as far as I know its not been introduced before.}
relates to numerical computing,  and focuses on approximating solutions by applying statistical and optimization methods. They are often data driven, and do not require an explicit algorithm to operate (beyond the indirect computations performed to approximate the result). The more relevant examples of \SuAI{} are \emph{Deep Learning} \citep{norvig2002modern} and \emph{Reinforcement Learning} \citep{sutton1998reinforcement}, but older methods such as \emph{Kalman Filters} \citep{simon2001kalman} or \emph{Monte Carlo Simulation} \citep{martin2024computing} could arguably also fall under this category.  \SuAI{} has recently grown in quality and proliferated to many applications such as image/language processing \citep{thapa2024application, vaswani2017attention}, and simulation \citep{jumper2021highly}. 

In order to leverage their respective strengths, there is a growing interest in studying and developing methods that merge Symbolic and Subsymbolic AI. This broad category, nicked \emph{\InAI{}}  \citep{Platzer_2024} (also referred to as neuro-symbolic AI \citep{d2009neural}, or hybrid intelligent systems \citep{medsker2012hybrid}), can range from applying logical principles in the architecture of \emph{ \NN{}s} (NN)  \citep{badreddine2022logic}, to using \SuAI{} to generate better heuristics for theorem provers \citep{laurent2022learning}. Even more, \InAI{} has found particular success in cyber-physical systems, where it often plays the role of a controller \citep{Platzer_2024}. 

Despite this, many \InAI{} systems often lack formal rigour, which prevents us from taking full advantage of its components. From the \SiAI{} perspective,  many symbolic principles are often haphazardly applied. Even more, many \InAI{} systems lack logical, algebraic or categorical characterizations, with proper syntax and semantics ( see, e.g., \cite{kaposi2024second}). Therefore, the interpretability and theoretical guarantees that symbolic systems benefit from end up being obfuscated. This is also exacerbated when dealing with \emph{\NN{}} (NN), widely considered to be black boxes \citep{buhrmester2021analysis}. On the other hand, the statistical and probabilistic guarantees that are given for certain \SuAI{}  systems (such as robustness \citep{casadio2022neural} and generalization error boundaries \citep{jakubovitz2019generalization} ), as well as the effects that \SiAI{} principles may have on performance, remain understudied for \InAI{}.

Being so, during my PhD I aim to aid in the development of formal theories and guarantees for \InAI{}, applying techniques from both the Symbolic and Subsymbolic AI communities. Our current approach focuses on the study of \emph{\QL{}s} (QL), i.e.~logics that model the real numbers. They function as a bridge between formal logic and machine learning, and may provide a proper logical interpretation for \InAI{} systems.

To illustrate QLs, let us have a toy syntax with atomic propositions and conjunction, such as
\begin{equation}
\begin{split}
    \Phi \ni \phi &:= A \,|\, \phi \land \phi
\end{split}
\end{equation}
where $A$ is interpreted in a domain $D \subseteq \Ereal$. $D$ varies among logics and restricts the interpretation of connectives. For example, the
G\"{o}del logic \citep{BAAZ200723} has a standard semantics over $[0, 1]$ where the conjunction is interpreted as the minimum function.

Recently, there was a surge of interest in QLs, stimulated by the growing interest in \emph{AI safety} \citep{dalrymple2024guaranteedsafeaiframework, davidad24}. \emph{\DL{}s} (DL) form a family of methods that apply key insights from QLs to this domain for property-driven learning and safe-by-construction \InAI{} \citep{FLINKOW2025103280, badreddine2022logic}. Nevertheless, there is one fundamental problem that DLs face: Many properties of interest for ML involve quantifiers, yet the majority of QLs are propositional \citep{bacci2023propositional, metcalfe2008proof, slusarz2023logic}.  Expanding some sound and complete propositional QLs to first-order logic often comes at the expense of either completeness or continuity, properties relevant for Symbolic and Subsymbolic AI, respectively.  For example, the first-order extension of GÃ¶del logic is the only one, among the most prominent fuzzy logics \cite{prooffuzzy,ldl}, that is sound and complete w.r.t. models with values in $[0,1]$ and with universal and existential quantifiers interpreted as infima and suprema \cite{BAAZ200723}. Recently, a promising solution was proposed by \citeauthor{capucci2025quantifiersquantitativereasoning}: Interpreting quantifiers as \textit{$p$-means} \citep{capucci2025quantifiersquantitativereasoning}, a generalization of $p$-norms over a probability space \citep{lpspaces}.  

By leveraging these results, we are currently developing a complete and continuous first-order DL with full theoretical rigour, to avoid the common shortcomings  of \InAI{} stated above. Furthermore, we seek to formalize our results in the Rocq proof assistant, with the aid of the \mathcomp{} library \citep{mathcomp}. In this progression review I present the state of the art of \yada, expand on our current approach, present some preliminary results,  and discuss my next steps. 

\jnote{Not sure what to put in the state of the art. The easiest thing to do would be to focus just on quantitative logics. But perhaps thats too limiting? I must also remember I must submit by the end of october...}