%% ----------------------------------------------------------------
%% Introduction.tex
%% ---------------------------------------------------------------- 
\section{Introduction} \label{Introduction}

\textbf{Neurosymbolic AI.} This project aims to contribute to the formal verification of \textit{Neurosymbolic Artificial Intelligence} (NeSy AI), that leverages the respective strengths of symbolic and learning approaches to computing \citep{ d2009neural, medsker2012hybrid, platzer2024intersymbolic}. The latter refers to \textit{Machine Learning} (ML), i.e. the study of  algorithms that learn approximate solutions from data \citep{brewka1996artificial}, characterized by \textit{Neural Networks} (NNs) \citep{badreddine2022logic, petersen2022deep}. The standard learning algorithms minimize a loss functions to optimize the NN's parameters to fit input-output vectors given by data \citep{brewka1996artificial}. 

\textbf{Quantitative logics and learning.} A popular approach for blending reasoning and learning is the real-valued interpretation of specifications written in a first-order language \citep{slusarz2023logic,serafini2016logic, petersen2022deep, choi2020probabilistic}. Therefore, many NeSy systems make implicit or explicit use of \textit{Quantitative Logics} (QLs), i.e. logics that model subsets of the real numbers. To illustrate QLs, let us have a toy syntax with atomic propositions and conjunction, such as
\begin{equation}
\begin{split}
    \Phi \ni \phi &:= A \,|\, \phi \land \phi
\end{split}
\end{equation}
where $\phi$ is interpreted through a mapping $\tempty{\cdot} : \Phi \rightarrow I$ such that $ \tempty{\phi} \in I \subseteq \Ereal$. A specification can be expressed in the syntax, and its mapping integrated into the system, either as a loss function \citep{slusarz2023logic,van2022analyzing} or as a computational graph \cite{serafini2016logic, petersen2022deep, choi2020probabilistic}, to penalize depending on how much the network deviates from the stated property. For example, a canonical specification of this kind is  \textit{robustness} \citep{casadio2022neural},  i.e. small perturbations to the inputs of a NN should result in small changes to its output.
\begin{definition}[$\epsilon$-$\delta$-Robustness] % no space here
\label{Robustness}%
    Let $\epsilon, \delta \in \real^+$, $||\cdot||$ be a norm, and $f : \real^n \rightarrow \real^m$ be a measurable function.
    One says \textit{$f$ is $\epsilon$-$\delta$-robust} around $\bar x \in \real ^ n$ if 
    \begin{equation}
    \label{eq:robustness}
        \forall x\in \real^n , ||x - \bar x|| \leq \epsilon \Rightarrow || 
			f(x) - f(\bar x)|| \leq \delta  
    \end{equation}
\end{definition}

Well known QLs such as \textit{Fuzzy Logics} \citep{cintula2011handbook} as well as novel logics developed by the ML community \citep{serafini2016logic, varnai2020robustness, fischer2019dl2} have been applied to this domain.

%\jnote{What are are they, mention fuzzy and quantale as examples. Mention how they differ: different semantics.}
%\jnote{Interest on integrating symbolic/background knowledge into learning: property driven training. First-order QLs have been implictly or explicitly used for that. Explain how, give robustness example. Mention how fuzzy logics have been used, and ML logics developed.}

%\textbf{Application to machine learning.}

\textbf{Programming language support.} The growing interest in QLs from the ML community has motivated their use to give programming language support for NeSy learning. In particular, QLs are useful for compiling specifications to the back-ends of NNs verifiers. As an example, Vehicle \citep{vehicle}, provides a higher-order typed specification language, and gives correct-by-construction translations into loss functions. This calls for stronger guarantees about the correctness of such compilers.


%\jnote{This has motivated the use of QLs to give programming language support for property driven training. Mention vehicle as an example. }


\textbf{Learning and logical properties.} We then would benefit from a QL that possesses both properties to facilitate learning, that we call \textit{Learning Properties}, and is well-behaved from a proof-theoretic perspective, that we call \textit{Logical Properties}. It is worth clarifying that a QL must not meet all  properties to be viable for learning or certification. Instead, they should be thought of as \textit{design choices}, giving different classes of logics. A more comprehensive list and formal definitions of these properties are given in \cref{Background}. Some relevant properties include:

\textbf{Learning Properties}
\begin{itemize}
%\item \textit{Differentiability}: Functions should be Lebesgue differentiable.
%\item \textit{Boundness}: Functions should satisfy boundary conditions.
\item \textit{Scale Invariance}: The behaviour of functions should not change when its inputs are scaled by a common factor.
%\item \textit{Weak Smoothness}: Functions should be continuous at points where there is a unique minimal term.
\item \textit{Shadow-lifting}: A function should improve if any of its parameters improves.
%\item \textit{Single-passing}: Functions should have nonzero derivatives on at most one input argument.
\item \textit{Non-vanishing gradients}: A function's gradient should not approximate zero at a sub-interval of the domain.
\end{itemize}

\textbf{Logical Properties}
\begin{itemize}
    %\item \textit{Involutivity}: Double negations should cancel out.
    \item \textit{Compositionality}: Negation should compose with conjunction and disjunction, while conjunction and disjunction should satisfy the usual properties of idempotence, commutativity, and associativity.
    \item \textit{Prelinearity}: The logic should possess a notion of total order. 
    \item \textit{Quantifier Aggregation}: Quantifiers should be monotonic, continuous, and satisfy boundary conditions.
\end{itemize}
On top of these properties, a logic should possess a calculus and formal semantics, such that it is \textit{Sound} and \textit{Complete}. 

%\jnote{List properties, intuitive definition}
%\textbf{How to ensure correctness.}
%\jnote{List properties, intuitive definition}
\textbf{An empty intersection of properties.} However, there is no first-order QL that possesses both good learning and logical properties. While established logics, such as fuzzy logics, are well understood and possess good logical properties, they have been shown to not be suited for learning \citep{van2022analyzing, affeldt2024taming, FLINKOW2025103280}. For instance, many are not continuous nor differentiable, which hinders performance. On the other hand, logics proposed by the ML community often lack a calculus or formal semantics, are not compositional, or their quantifiers are not aggregators \citep{van2022analyzing, affeldt2024taming}. On top of this, it has been proven that a QL cannot meet shadow-lifting while being associative and idempotent \citep{varnai2020robustness}.

\textbf{A novel approach.} Given the provided information, can we develop a first-order QL with good learning and logical properties?   Recently, a promising approach for first-order QLs was proposed by \citeauthor{capucci2024quantifiers}: 
Interpreting implication as division, and existential quantifiers as \textit{p-means} \citep{capucci2024quantifiers}. These are a family of norms parametrized by a positive real $p$, that approximates the maximum when $p$ tends to infinity, resembling Yager fuzzy logic \citep{cintula2011handbook}. In this way $p$ can be seen as a "softness" modality that balances shadow-lifting and idempotence \citep{capucci2024quantifiers}. We build on these ideas to develop a novel, well-behaved QL, that we call \emph{\OurLogic} (\OL). Unlike \citeauthor{capucci2024quantifiers}, who uses a deep-inference inspired framework \citep{guglielmi2007system, guglielmi2015deep}, our approach follows the subtructural logic tradition \citep{galatos2007residuated}, taking bunches from the \textit{Logic of Bunched Implications} \citep{o1999logic}  and hypersequents from fuzzy logics \citep{cintula2011handbook, prooffuzzy}. To guarantee the correctness of our results, we formalize using the \textit{Mathematical Components library} (Mathcomp) in the Rocq proof assistant \citep{mathcomp}, in which numerous results from algebra and real analysis have been mechanized. We seek this formalization to aid in the development of programming language support for the verification of NeSy AI \citep{vehicle}. 

\subsection{Research Questions}

Here we present our main research questions, as well as our approach for answering them. Our high-level goal is to develop, study and formalize a first-order QL with good learning and logical properties. To this aim, we divide our research into nine research questions:

\begin{enumerate}
    \item \textbf{Syntax and real-valued semantics.} Can we develop a first-order syntax and real-valued semantics with good learning and logical properties? 
    
    We borrow directly from \citeauthor{capucci2024quantifiers} the real-valued semantics of formulas, known to meet most logical properties \citep{capucci2024quantifiers}. On top of this, the interpretation is known to be scale invariant, while shadow-lifting is obtained through the softness modality. It remains unclear under what conditions other properties are met, such as weak smoothness, or non-vanishing gradients. 
    
    \item \textbf{Sound propositional calculus.} Can we develop a sound propositional calculus for \OL{}?
    
    Our current version of \OL{} could be summarized as a soft, bunched, hypersequent calculus. Unlike \citeauthor{capucci2024quantifiers}, we use the same notion of validity as substructural logics \citep{galatos2007residuated}, and we extend the real-valued interpretation to deal with bunches and hypersequents. \OL{} has been proven sound with respect to its real-valued interpretation. 

    \item \textbf{Algebraic semantics.} Can we develop sound algebraic semantics for \OL{}?
    
    Given its relation to substructural logics, \OL{} semantics could be given by a residuated lattice \citep{galatos2007residuated}, extended to deal with the softness modality (similarly to how algebraic semantics for linear logic are extended to deal with exponentials \citep{agliano2025algebraic}).
    
    \item \textbf{Complete propositional calculus.} Can we prove propositional \OL{} complete?
    
    A first approximation to completeness is to show that a collection of formulas, known to be valid in the semantics, are provable in the calculus. While this does not guarantee completeness, it is an informative, necessary condition. Then, we hope to leverage \OL{}'s relation to substructural logics to prove completeness with respect to its algebraic semantics (and therefore, also with respect to its real-valued intepretation), applying known techniques from substructual logics.

     \item \textbf{Propositional formalization.} Can we formalize propositional \OL{} and its properties?

    We make use of Mathcomp, building on previous work by \citeauthor{affeldt2024taming} \citep{affeldt2024taming}. We aim to adapt this formalization for \OL{} and its properties, as well as soundness and completeness. To achieve this, we are currently extending Mathcomp's implementation of the extended real numbers. 
    
    \item \textbf{First-order extension.} Can \OL{}'s calculus be extended into first-order, while maintaining soundness, as well as good learning and logical properties?

    Extending our logic into first-order requires modifying our real-value interpretation, and therefore our algebraic semantics. For our current approach to maintain soundness, additional conditions must be imposed on substitution. On the other hand, it remains unclear if learning properties are maintained. 
    
    \item \textbf{Complete predicate calculus.} Can we prove our predicate \OL{} complete?
    
    Similar approach to the propositional case. 

    \item \textbf{Predicate formalization.} Can we formalize first-order \OL{} and its properties?

    Formalization of the first-order case comes with the additional challenge of quantifier semantics, which involve results from real analysis and probability. Most notably, measure spaces, probability spaces, and Lebsegue integrals, as well as the use of results such as Jensen's  and Hölder's inequalities \citep{mitrinovic1970analytic}. Mathcomp is a particularly good fit for this task, due to its extensive mathematical libraries. Many of the aforementioned standard results from measure theory are formalized in the library modules on algebra and analysis. However, some, such as the encoding of extended real numbers, still require further development.
    
    \item \textbf{Practicality.} Does \OL{} offer some insight on property-driven training?

    We aim to supplement our theoretical results with experimental evidence. This includes evaluating how easily our results can be translated into actual implementations, as well as the relevance of the learning properties.
    
\end{enumerate}